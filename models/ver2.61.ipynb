{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the last part-pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(1)\n",
    "# Data Processing and Visualization\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Image Processing and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "\n",
    "# Deep Learning Framework (TensorFlow/Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Activation, Conv2DTranspose, Concatenate, Input\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Specific TensorFlow/Keras Components\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ReduceLROnPlateau, \n",
    "    EarlyStopping, \n",
    "    ModelCheckpoint, \n",
    "    LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, normalize\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datamap imports the path of images and store them to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2)\n",
    "datamap = []\n",
    "\n",
    "# Specify the path to your dataset on your Windows machine\n",
    "dataset_path = \"Dataset_MRI\"\n",
    "\n",
    "# Iterate over subdirectories in the specified path\n",
    "for sub_dir_path in glob.glob(dataset_path + \"/*\"):\n",
    "    # Check if the current path is a directory\n",
    "    if os.path.isdir(sub_dir_path):\n",
    "        # Extract the tumor type from the directory name\n",
    "        dir_name = os.path.basename(sub_dir_path)\n",
    "\n",
    "        # Iterate over files in the directory\n",
    "        for filename in os.listdir(sub_dir_path):\n",
    "            # Construct the full path to the image file\n",
    "            image_path = os.path.join(sub_dir_path, filename)\n",
    "            \n",
    "            # Load the image as an array\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            # Check if the image is successfully loaded\n",
    "            if image is not None:\n",
    "                # Append a dictionary to the datamap\n",
    "                datamap.append({\n",
    "                    'tumor_type': dir_name,\n",
    "                    'file_name': filename,\n",
    "                    'file_path': image_path,  # Include the full file path\n",
    "                    'image': image\n",
    "                })\n",
    "\n",
    "# Display the datamap\n",
    "for entry in datamap[:10]:  # Display only the first 10 entries\n",
    "    print(f\"Tumor Type: {entry['tumor_type']}, File Name: {entry['file_name']}, File Path: {entry['file_path']}, Image Shape: {entry['image'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverting the datamap to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "(3)\n",
    "# Assume data_map is already filled with the image data as shown in the previous steps\n",
    "# Convert the data_map to a pandas DataFrame\n",
    "datamap_df = pd.DataFrame(datamap)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify its structure\n",
    "print(datamap_df.head())\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the images before pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num_rows, num_cols, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Select random rows from the DataFrame\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m random_rows \u001b[38;5;241m=\u001b[39m \u001b[43mdatamap_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images_to_display\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Iterate through the randomly selected rows to display images\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (index, row) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(random_rows\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Determine the current row and column for the subplot\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:6029\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[0;32m   6026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6027\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[1;32m-> 6029\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6030\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   6032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[1;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    154\u001b[0m )\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:945\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(4)\n",
    "# Fixed number of images to display and columns\n",
    "num_images_to_display = 9\n",
    "num_cols = 3\n",
    "num_rows = 3  # Since num_images_to_display is 9 and num_cols is 3\n",
    "\n",
    "# Create a new figure with subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
    "\n",
    "# Select random rows from the DataFrame\n",
    "random_rows = datamap_df.sample(n=num_images_to_display)\n",
    "\n",
    "# Iterate through the randomly selected rows to display images\n",
    "for idx, (index, row) in enumerate(random_rows.iterrows()):\n",
    "    # Determine the current row and column for the subplot\n",
    "    current_row, current_col = divmod(idx, num_cols)\n",
    "\n",
    "    # Load and display the image\n",
    "    img = Image.open(row['file_path'])\n",
    "    axes[current_row, current_col].imshow(img)\n",
    "    axes[current_row, current_col].set_title(row['file_name'])\n",
    "    axes[current_row, current_col].axis('off')\n",
    "\n",
    "# Hide any empty subplots if necessary\n",
    "for i in range(idx + 1, num_rows * num_cols):\n",
    "    axes[i // num_cols, i % num_cols].axis('off')\n",
    "\n",
    "# Adjust subplot spacing and display the figure\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, testing, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_df: 14\n",
      "Size of val_df: 4\n"
     ]
    }
   ],
   "source": [
    "(5)\n",
    "# Import necessary libraries for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "train_df, val_df = train_test_split(datamap_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure that the file paths in the DataFrame are strings\n",
    "train_df['file_path'] = train_df['file_path'].astype(str)\n",
    "val_df['file_path'] = val_df['file_path'].astype(str)\n",
    "\n",
    "# train_df and val_df are now ready for use\n",
    "print(\"Size of train_df:\", len(train_df))\n",
    "print(\"Size of val_df:\", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels in the training set\n",
    "train_df['encoded_labels'] = label_encoder.fit_transform(train_df['tumor_type'])\n",
    "\n",
    "# Transform the labels in the validation set\n",
    "val_df['encoded_labels'] = label_encoder.transform(val_df['tumor_type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, image_size, num_classes, augmenter=None, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.augmenter = augmenter  # For image augmentation\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_image_paths = [self.image_paths[k] for k in batch_indices]\n",
    "        batch_labels = [self.labels[k] for k in batch_indices]\n",
    "        X, y = self.__generate_Xy(batch_image_paths, batch_labels)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __generate_Xy(self, batch_image_paths, batch_labels):\n",
    "        X = np.empty((self.batch_size, *self.image_size, 3))  # Always create a 3-channel image\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "\n",
    "        for i, (path, label) in enumerate(zip(batch_image_paths, batch_labels)):\n",
    "            image = cv2.imread(path)\n",
    "\n",
    "            # Ensure the image has 3 channels (convert grayscale to RGB if needed)\n",
    "            if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            # Resize the image to the specified size\n",
    "            image = cv2.resize(image, self.image_size)\n",
    "\n",
    "            # Normalize the image and store it in X\n",
    "            X[i,] = image / 255.0  # Normalize here\n",
    "\n",
    "            y[i,] = tf.keras.utils.to_categorical(label, self.num_classes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        # Check if the image is grayscale (single channel)\n",
    "        if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "            # Convert grayscale to RGB by duplicating the channel\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            # Assuming the image is already in RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply OTSU's thresholding\n",
    "        thresholded = apply_otsus_thresholding(image)\n",
    "\n",
    "        # Resizing\n",
    "        resized = resize_image(thresholded, self.image_size)\n",
    "\n",
    "        # Image Augmentation (if augmenter is provided)\n",
    "        if self.augmenter:\n",
    "            resized = self.augmenter(resized)\n",
    "\n",
    "        # Image Normalization\n",
    "        normalized = resized / 255.0\n",
    "\n",
    "        return normalized\n",
    "\n",
    "# Helper preprocessing functions (add or modify as needed)\n",
    "def apply_otsus_thresholding(image):\n",
    "    # Convert to grayscale for thresholding\n",
    "    grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, thresholded = cv2.threshold(grayscale, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresholded\n",
    "\n",
    "def resize_image(image, size=(256, 256)):\n",
    "    return cv2.resize(image, size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(7)\n",
    "# Parameters for the data generator\n",
    "batch_size = 32  # Example batch size, adjust as needed\n",
    "image_size = (256, 256)  # Target image size as per the model input\n",
    "num_classes = 7  # Adjust based on the number of tumor types or categories\n",
    "\n",
    "# Instantiate the custom data generators\n",
    "# Instantiate the custom data generators with encoded labels\n",
    "train_generator = CustomDataGenerator(\n",
    "    image_paths=train_df['file_path'].tolist(),\n",
    "    labels=train_df['encoded_labels'].tolist(),  # Use encoded labels\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    num_classes=num_classes,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = CustomDataGenerator(\n",
    "    image_paths=val_df['file_path'].tolist(),\n",
    "    labels=val_df['encoded_labels'].tolist(),  # Use encoded labels\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    num_classes=num_classes,\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Data & Validation-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"Custom_RGB_Brain_Tumor_Segmentation_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 256)          590080    ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['conv2d_6[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 512)          2359808   ['conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 32, 32, 512)          2359808   ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['conv2d_9[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 16, 16, 512)          2359808   ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 16, 16, 512)          2359808   ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 16, 16, 512)          2359808   ['conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 32, 32, 512)          1049088   ['conv2d_12[0][0]']           \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 32, 32, 1024)         0         ['conv2d_transpose[0][0]',    \n",
      "                                                                     'conv2d_9[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 32, 32, 1024)         4096      ['concatenate[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 32, 32, 1024)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 512)          4719104   ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 32, 32, 512)          2359808   ['conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 32, 32, 512)          0         ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 64, 64, 256)          524544    ['dropout[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_transpose_1[0][0]',  \n",
      " )                                                                   'conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 64, 64, 512)          2048      ['concatenate_1[0][0]']       \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 64, 64, 512)          0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 256)          1179904   ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 64, 64, 256)          590080    ['conv2d_15[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 64, 64, 256)          0         ['conv2d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 64, 64, 128)          32896     ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['dropout_1[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 128, 128, 128)        0         ['conv2d_17[0][0]']           \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
      " )                                                                   'up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 128, 128, 256)        1024      ['concatenate_2[0][0]']       \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 128, 128, 256)        0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 128, 128, 128)        295040    ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 128, 128, 128)        147584    ['conv2d_18[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 128, 128, 128)        0         ['conv2d_19[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['dropout_2[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 256, 256, 64)         256       ['conv2d_transpose_3[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 256, 256, 64)         36928     ['conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 256, 256, 64)         0         ['conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 256, 256, 7)          455       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25858503 (98.64 MB)\n",
      "Trainable params: 25854791 (98.63 MB)\n",
      "Non-trainable params: 3712 (14.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(8)\n",
    "# Custom VGG16 Encoder\n",
    "def custom_vgg16_encoder(input_tensor):\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_tensor)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    s1 = x  # Save the feature map for decoder\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    s2 = x  # Save the feature map for decoder\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    s3 = x  # Save the feature map for decoder\n",
    "\n",
    "    return s3, s2, s1\n",
    "\n",
    "# Custom U-Net Decoder\n",
    "def custom_unet_decoder(conv4, conv3, conv2, conv1):\n",
    "    # Decoder\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(conv4)\n",
    "    u6 = layers.concatenate([u6, conv3], axis=-1)\n",
    "    u6 = layers.BatchNormalization()(u6)\n",
    "    u6 = layers.Activation('relu')(u6)\n",
    "    u6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    u6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    u6 = layers.Dropout(0.5)(u6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(u6)\n",
    "    u7 = layers.concatenate([u7, conv2], axis=-1)\n",
    "    u7 = layers.BatchNormalization()(u7)\n",
    "    u7 = layers.Activation('relu')(u7)\n",
    "    u7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    u7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    u7 = layers.Dropout(0.5)(u7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(u7)\n",
    "\n",
    "    # Adjust the shape of conv1 to match the shape of u8\n",
    "    conv1_adjusted = layers.Conv2D(128, (1, 1), activation='relu')(conv1)\n",
    "    conv1_adjusted = layers.UpSampling2D(size=(2, 2))(conv1_adjusted)\n",
    "\n",
    "    u8 = layers.concatenate([u8, conv1_adjusted], axis=-1)\n",
    "    u8 = layers.BatchNormalization()(u8)\n",
    "    u8 = layers.Activation('relu')(u8)\n",
    "    u8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    u8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    u8 = layers.Dropout(0.5)(u8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u8)\n",
    "    u9 = layers.BatchNormalization()(u9)\n",
    "    u9 = layers.Activation('relu')(u9)\n",
    "    u9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    u9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    u9 = layers.Dropout(0.5)(u9)\n",
    "    return u9\n",
    "\n",
    "# Build Custom UNet-VGG16 Model\n",
    "def build_custom_unet_vgg16_for_rgb(input_shape, num_classes=7):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s3, s2, s1 = custom_vgg16_encoder(inputs)\n",
    "\n",
    "    \"\"\" Decoder (U-Net) \"\"\"\n",
    "    u_net_decoder_output = custom_unet_decoder(s3, s2, s1, s1)\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(u_net_decoder_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs, name=\"Custom_RGB_Brain_Tumor_Segmentation_Model\")\n",
    "    return model\n",
    "\n",
    "# Create the model instance for RGB input\n",
    "custom_rgb_model = build_custom_unet_vgg16_for_rgb((256, 256, 3), num_classes=7)\n",
    "\n",
    "# Compile the model\n",
    "custom_rgb_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "custom_rgb_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(9)\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "\n",
    "def categorical_tversky_index(y_true, y_pred, alpha=0.7):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "\n",
    "    true_positives = K.sum(y_true * y_pred, axis=-1)\n",
    "    false_negatives = K.sum(y_true * (1 - y_pred), axis=-1)\n",
    "    false_positives = K.sum((1 - y_true) * y_pred, axis=-1)\n",
    "\n",
    "    return (true_positives + smooth) / (true_positives + alpha * false_negatives + (1 - alpha) * false_positives + smooth)\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
    "    tversky_index = categorical_tversky_index(y_true, y_pred)\n",
    "    return K.pow((1 - tversky_index), gamma)\n",
    "\n",
    "# Custom loss function\n",
    "def modified_categorical_crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred) + (1 - categorical_tversky_index(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10)\n",
    "# Custom loss function\n",
    "def modified_categorical_crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred) + (1 - categorical_tversky_index(y_true, y_pred))\n",
    "\n",
    "# Optimizer\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.05, epsilon=0.1)\n",
    "\n",
    "# Compile the model\n",
    "custom_rgb_model.compile(optimizer=adam,\n",
    "                     loss=modified_categorical_crossentropy,\n",
    "                     metrics=['accuracy']) \n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "# Define a custom callback to save the entire training history\n",
    "class SaveHistoryCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, csv_filename=\"training_history.csv\"):\n",
    "        self.epoch_history = []\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_history.append(logs)\n",
    "        df = pd.DataFrame(self.epoch_history)\n",
    "        df.to_csv(self.csv_filename, index=False)\n",
    "\n",
    "save_history_callback = SaveHistoryCallback(csv_filename=\"weights_model/training_history.csv\")\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights_model/brain_tumor_model_weights.hdf5\",\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001, factor=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Temp\\ipykernel_6320\\225655208.py\", line 4, in modified_categorical_crossentropy  *\n        return tf.keras.losses.categorical_crossentropy(y_true, y_pred) + (1 - categorical_tversky_index(y_true, y_pred))\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, None, None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m (\u001b[38;5;241m11\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Train the model with the callbacks\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_rgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_history_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Access the entire training history\u001b[39;00m\n\u001b[0;32m     12\u001b[0m entire_training_history \u001b[38;5;241m=\u001b[39m save_history_callback\u001b[38;5;241m.\u001b[39mepoch_history\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetre4mhpd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegdcn3sv5.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__modified_categorical_crossentropy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcategorical_crossentropy, (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(categorical_tversky_index), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Temp\\ipykernel_6320\\225655208.py\", line 4, in modified_categorical_crossentropy  *\n        return tf.keras.losses.categorical_crossentropy(y_true, y_pred) + (1 - categorical_tversky_index(y_true, y_pred))\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Implici7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, None, None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "(11)\n",
    "# Train the model with the callbacks\n",
    "history = custom_rgb_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, checkpointer, reduce_lr, save_history_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Access the entire training history\n",
    "entire_training_history = save_history_callback.epoch_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
